{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2c616e39",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir(\"/Users/auguste/Desktop/Dossiers/HEC/Courses/RP/rp-adaptive-ml-trading/data/raw/AAPL\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "027f601b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "\n",
    "# Display settings\n",
    "pd.set_option('display.max_columns', 100)\n",
    "pd.set_option('display.width', 150)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4b378687",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Valuation data cleaned and ready to merge.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load normally (row 0 is the header)\n",
    "valuation = pd.read_csv(\"AAPL_monthly_valuation_measures_cleaned.csv\")\n",
    "\n",
    "# Drop the first data row if it contains 'ttm' (non-date value)\n",
    "valuation = valuation[~valuation[\"date\"].str.lower().str.contains(\"ttm\", na=False)]\n",
    "\n",
    "# Drop the last two mostly-empty columns, if present\n",
    "if valuation.shape[1] > 11:\n",
    "    valuation = valuation.iloc[:, :11]\n",
    "\n",
    "# Convert 'date' to datetime\n",
    "valuation[\"date\"] = pd.to_datetime(valuation[\"date\"], errors=\"coerce\")\n",
    "valuation = valuation.dropna(subset=[\"date\"])\n",
    "\n",
    "# Clean numeric fields\n",
    "for col in valuation.columns:\n",
    "    if col not in [\"date\", \"ticker\"]:\n",
    "        valuation[col] = (\n",
    "            valuation[col].astype(str)\n",
    "            .str.replace(\",\", \"\", regex=False)\n",
    "            .replace(\"\", pd.NA)\n",
    "            .astype(float)\n",
    "        )\n",
    "\n",
    "# Normalize date\n",
    "valuation[\"date\"] = valuation[\"date\"].dt.normalize()\n",
    "\n",
    "# Sort and drop duplicate dates\n",
    "valuation = valuation.sort_values(\"date\").drop_duplicates(subset=\"date\", keep=\"last\")\n",
    "# Filter for dates between 2014 and 2025 inclusive\n",
    "valuation = valuation[\n",
    "    (valuation[\"date\"] >= \"2014-01-01\") & (valuation[\"date\"] <= \"2024-12-31\")\n",
    "]\n",
    "# Drop unwanted columns\n",
    "valuation = valuation.drop(columns=[\"EnterprisesValueRevenueRatio\", \"EnterprisesValueEBITDARatio\"], errors='ignore')\n",
    "\n",
    "print(\"✅ Valuation data cleaned and ready to merge.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "91e5190f",
   "metadata": {},
   "outputs": [],
   "source": [
    "valuation.to_csv(\"AAPL_valuations.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c67063cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Cleaned options data and engineered sentiment features.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from dateutil import parser\n",
    "\n",
    "# --- Load options data ---\n",
    "opt = pd.read_csv(\"AAPL_options_all.csv\")\n",
    "\n",
    "# Manual renaming of critical columns to avoid double underscore issues\n",
    "rename_map = {\n",
    "    'last_trade_date_(edt)': 'last_trade_date',\n",
    "    'expiry_date': 'expiry_date',\n",
    "    'contract_name': 'contract_name',\n",
    "    'open_interest': 'open_interest',\n",
    "    'volume': 'volume',\n",
    "    'implied_volatility': 'implied_volatility'\n",
    "}\n",
    "opt.rename(columns=rename_map, inplace=True)\n",
    "\n",
    "# Strip and standardize all remaining column names\n",
    "opt.columns = (\n",
    "    opt.columns\n",
    "    .str.strip()\n",
    "    .str.lower()\n",
    "    .str.replace(r'[^\\w]+', '_', regex=True)\n",
    "    .str.replace(r'_+', '_', regex=True)\n",
    "    .str.strip('_')\n",
    ")\n",
    "\n",
    "# --- Parse dates robustly ---\n",
    "def parse_mixed_datetime(val):\n",
    "    if pd.isna(val):\n",
    "        return pd.NaT\n",
    "    try:\n",
    "        return pd.to_datetime(val, format='%m/%d/%Y %I:%M %p')  # Try US format first\n",
    "    except Exception:\n",
    "        try:\n",
    "            return pd.to_datetime(val)  # Fallback to ISO/generic\n",
    "        except Exception:\n",
    "            return pd.NaT\n",
    "\n",
    "opt['last_trade_date'] = opt['last_trade_date'].apply(parse_mixed_datetime)\n",
    "opt['expiry_date'] = pd.to_datetime(opt['expiry_date'], errors='coerce')\n",
    "\n",
    "# --- Filter and clean ---\n",
    "opt = opt.dropna(subset=['last_trade_date', 'expiry_date'])\n",
    "opt = opt[opt['contract_name'].str.contains(\"AAPL\", na=False)]\n",
    "\n",
    "# Convert numerics\n",
    "opt['open_interest'] = pd.to_numeric(opt['open_interest'], errors='coerce')\n",
    "opt['volume'] = pd.to_numeric(opt['volume'], errors='coerce')\n",
    "opt['implied_volatility'] = (\n",
    "    pd.to_numeric(opt['implied_volatility'].astype(str).str.replace('%', ''), errors='coerce') / 100\n",
    ")\n",
    "\n",
    "# Extract date, option type, DTE\n",
    "opt['option_type'] = opt['contract_name'].str.extract(r'(\\d+[CP])')[0].str[-1].map({'C': 'call', 'P': 'put'})\n",
    "opt['dte'] = (opt['expiry_date'] - opt['last_trade_date']).dt.days\n",
    "opt['date'] = opt['last_trade_date'].dt.floor('D')\n",
    "\n",
    "def tag_dte_bucket(dte):\n",
    "    if 0 <= dte <= 10:\n",
    "        return 'short'\n",
    "    elif 20 <= dte <= 40:\n",
    "        return 'medium'\n",
    "    elif 50 <= dte <= 150:\n",
    "        return 'long'\n",
    "    return None\n",
    "\n",
    "opt['dte_bucket'] = opt['dte'].apply(tag_dte_bucket)\n",
    "opt = opt[opt['dte_bucket'].notnull()]\n",
    "\n",
    "# --- Done ---\n",
    "print(\"✅ Cleaned options data and engineered sentiment features.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "3d677f1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "opt.to_csv(\"AAPL_options_vdx.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3b04337f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/auguste/Desktop/Dossiers/HEC/Courses/RP/rp-adaptive-ml-trading/data/raw/AAPL'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2f395763",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/g9/jl77b7s511d2m50h489cqhwr0000gp/T/ipykernel_82382/1625377851.py:12: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  valuation[\"date\"] = pd.to_datetime(valuation[\"date\"], errors=\"coerce\")\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# --- Load Datasets ---\n",
    "daily_factors = pd.read_csv(\"merged_daily_factors_final.csv\", parse_dates=[\"date\"])\n",
    "technicals = pd.read_csv(\"AAPL_2014_2024_technical_cleaned.csv\", parse_dates=[\"date\"])\n",
    "valuation = pd.read_csv(\n",
    "    \"AAPL_monthly_valuation_measures_cleaned.csv\",\n",
    "    parse_dates=[\"date\"],\n",
    "    date_format=\"%m/%d/%Y\",\n",
    ")\n",
    "# Optional: if 'date' column is still object type, parse explicitly\n",
    "valuation[\"date\"] = pd.to_datetime(valuation[\"date\"], errors=\"coerce\")\n",
    "valuation = valuation.dropna(subset=[\"date\"]).sort_values(\"date\")\n",
    "# Collect in list\n",
    "dfs = [daily_factors, technicals, valuation]\n",
    "\n",
    "# Normalize dates and remove timezones\n",
    "# Normalize and remove timezone safely\n",
    "for df in dfs:\n",
    "    df[\"date\"] = pd.to_datetime(df[\"date\"])\n",
    "    if df[\"date\"].dt.tz is not None:\n",
    "        df[\"date\"] = df[\"date\"].dt.tz_localize(None)\n",
    "    df[\"date\"] = df[\"date\"].dt.normalize()\n",
    "\n",
    "# Drop true duplicate dates (keep last seen)\n",
    "for i in range(len(dfs)):\n",
    "    dfs[i] = dfs[i].drop_duplicates(subset=\"date\", keep=\"last\")\n",
    "\n",
    "# Merge everything into one unified DataFrame\n",
    "merged_df = dfs[0]\n",
    "for df in dfs[1:]:\n",
    "    merged_df = pd.merge(merged_df, df, on=\"date\", how=\"left\", suffixes=('', '_dup'))\n",
    "\n",
    "# Optional: Handle _dup columns if any\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f2a5cb63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset 0 preview:\n",
      "0   2014-01-02\n",
      "1   2014-01-03\n",
      "2   2014-01-06\n",
      "3   2014-01-07\n",
      "4   2014-01-08\n",
      "Name: date, dtype: datetime64[ns]\n",
      "Dataset 1 preview:\n",
      "0   2014-01-01\n",
      "1   2014-01-02\n",
      "2   2014-01-05\n",
      "3   2014-01-06\n",
      "4   2014-01-07\n",
      "Name: date, dtype: datetime64[ns]\n",
      "Dataset 2 preview:\n",
      "498   1985-08-31\n",
      "497   1985-09-30\n",
      "496   1985-10-31\n",
      "495   1985-11-30\n",
      "494   1985-12-31\n",
      "Name: date, dtype: datetime64[ns]\n"
     ]
    }
   ],
   "source": [
    "for i, df in enumerate(dfs):\n",
    "    print(f\"Dataset {i} preview:\")\n",
    "    print(df[\"date\"].head(5))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba075dd3",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5044c1c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️ Found 0 duplicate date(s) in valuation data.\n",
      "⚠️ Skipped timezone strip: 'date' column missing in <class 'pandas.core.frame.DataFrame'>\n",
      "✅ Unified dataset ready.\n",
      "Date range: 2014-01-01 to 2025-01-01\n",
      "Shape: (5559, 233)\n",
      "Columns: ['date', 'ff3_mkt_rf', 'ff3_smb', 'ff3_hml', 'ff3_rf', 'ff3_mom', 'ff5_mkt_rf', 'ff5_smb', 'ff5_hml', 'ff5_rmw']... (+223 more)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/g9/jl77b7s511d2m50h489cqhwr0000gp/T/ipykernel_82382/2681465812.py:59: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  valuation[\"date\"] = pd.to_datetime(valuation[\"date\"], errors=\"coerce\")\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# --- Load Datasets ---\n",
    "daily_factors = pd.read_csv(\"merged_daily_factors_final.csv\", parse_dates=[\"date\"])\n",
    "technicals = pd.read_csv(\"AAPL_2014_2024_technical_cleaned.csv\", parse_dates=[\"date\"])\n",
    "valuation = pd.read_csv(\n",
    "    \"AAPL_monthly_valuation_measures_cleaned.csv\",\n",
    "    parse_dates=[\"date\"],\n",
    "    date_format=\"%m/%d/%Y\"\n",
    ")\n",
    "\n",
    "# --- Clean Technicals ---\n",
    "def clean_technicals(df):\n",
    "    df.columns = (\n",
    "        df.columns\n",
    "        .str.strip()\n",
    "        .str.lower()\n",
    "        .str.replace(r\"[^\\w]+\", \"_\", regex=True)\n",
    "        .str.replace(r\"_+\", \"_\", regex=True)\n",
    "        .str.strip(\"_\")\n",
    "    )\n",
    "    rename_map = {\n",
    "        'rsi_rsi_14': 'rsi',\n",
    "        'macd_macd_12_26_9': 'macd',\n",
    "        'signal_macd_12_26_9': 'macd_signal',\n",
    "        'macd_12_26_9_hist': 'macd_hist',\n",
    "        'result_schaff_10_23_50_ema': 'schaff_trend',\n",
    "        'result_w_acc_dist_n': 'acc_dist',\n",
    "        'result_ultimate_7_14_28': 'ultimate_osc',\n",
    "        'result_trix_14': 'trix',\n",
    "        'result_williams_r_14': 'williams_r',\n",
    "        'result_vol_roc_14': 'vol_roc',\n",
    "        'result_vol_osc_12_26_points': 'vol_osc',\n",
    "        'rel_vol_rel_vol_10_14': 'rel_vol',\n",
    "        'rel_vig_rel_vig_10': 'rel_vig',\n",
    "        'relvigsignal_rel_vig_10': 'rel_vig_signal',\n",
    "        'rel_vig_10_hist': 'rel_vig_hist',\n",
    "        'ravi_vdma_7_65_hist': 'ravi',\n",
    "        'result_psar_0_02_0_2': 'psar',\n",
    "        'result_on_bal_vol': 'obv',\n",
    "        'result_price_roc_14': 'roc',\n",
    "        'result_price_osc_12_26_ema_points': 'price_osc',\n",
    "        'result_chande_mtm_9': 'chande_mtm',\n",
    "        'result_cci_20': 'cci',\n",
    "        'result_m_flow_14': 'money_flow',\n",
    "        'result_momentum_14': 'momentum',\n",
    "        'result_med_price_14': 'median_price',\n",
    "        'result_std_dev_14_2_ma': 'std_dev',\n",
    "        'result_hist_vol_10_252_1': 'hist_vol',\n",
    "        'result_perf_idx_120_spy': 'perf_idx_spy',\n",
    "        'result_beta_20_spy': 'beta_spy',\n",
    "    }\n",
    "    return df.rename(columns=rename_map)\n",
    "\n",
    "technicals = clean_technicals(technicals)\n",
    "\n",
    "# --- Prepare Valuation (monthly → daily ffill) ---\n",
    "valuation[\"date\"] = pd.to_datetime(valuation[\"date\"], errors=\"coerce\")\n",
    "valuation = valuation.dropna(subset=[\"date\"])\n",
    "\n",
    "# --- Diagnostic: Duplicated dates\n",
    "dup_counts = valuation['date'].value_counts()\n",
    "duplicate_dates = dup_counts[dup_counts > 1]\n",
    "print(f\"⚠️ Found {len(duplicate_dates)} duplicate date(s) in valuation data.\")\n",
    "if not duplicate_dates.empty:\n",
    "    print(duplicate_dates)\n",
    "\n",
    "# --- Safely aggregate duplicates by averaging numerics\n",
    "valuation = (\n",
    "    valuation.groupby(\"date\", as_index=False)\n",
    "             .mean(numeric_only=True)\n",
    ")\n",
    "\n",
    "# --- Resample to daily frequency\n",
    "valuation_daily = (\n",
    "    valuation.set_index(\"date\")\n",
    "             .sort_index()\n",
    "             .resample(\"D\")\n",
    "             .ffill()\n",
    "             .reset_index()\n",
    ")\n",
    "\n",
    "# --- Strip timezones from all date columns\n",
    "for df_ in [daily_factors, technicals, valuation_daily, options]:\n",
    "    if 'date' in df_.columns:\n",
    "        df_[\"date\"] = pd.to_datetime(df_[\"date\"]).dt.tz_localize(None)\n",
    "    else:\n",
    "        print(f\"⚠️ Skipped timezone strip: 'date' column missing in {type(df_)}\")\n",
    "\n",
    "# --- Merge All Datasets ---\n",
    "df = (\n",
    "    daily_factors\n",
    "    .merge(technicals, on=\"date\", how=\"outer\", suffixes=('', '_tech'))\n",
    "    .merge(valuation_daily, on=\"date\", how=\"left\", suffixes=('', '_val'))\n",
    "    .sort_values(\"date\")\n",
    "    .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "# --- Final Check ---\n",
    "print(\"✅ Unified dataset ready.\")\n",
    "print(f\"Date range: {df['date'].min().date()} to {df['date'].max().date()}\")\n",
    "print(f\"Shape: {df.shape}\")\n",
    "print(f\"Columns: {df.columns.tolist()[:10]}... (+{len(df.columns) - 10} more)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "80d1339d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"AAPL_unified_dataset.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "51586be6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📄 Saved: AAPL_unified_dataset.csv\n"
     ]
    }
   ],
   "source": [
    "# Load the Parquet file\n",
    "df_parquet = pd.read_parquet(\"AAPL_unified_dataset.parquet\")\n",
    "\n",
    "# Save to CSV for inspection\n",
    "df_parquet.to_csv(\"AAPL_unified_dataset.csv\", index=False)\n",
    "\n",
    "print(\"📄 Saved: AAPL_unified_dataset.csv\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rp-adaptive-ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
